
import json
import numpy as np
import torch
from bert_score import BERTScorer
from collections import defaultdict
import jieba
import re

class MedicalReportEvaluator:
    def __init__(self, model_type="bert-base-chinese"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        print("åˆå§‹åŒ–BERTScoreè¯„ä¼°å™¨...")
        self.bertscorer = BERTScorer(model_type=model_type, lang="zh")
        
        # åŒ»ç–—å®ä½“ç±»åˆ«
        self.entity_categories = {
            'symptoms': ['å’³å—½', 'å‘çƒ­', 'å‘çƒ§', 'æµæ¶•', 'é¼»å¡', 'å‘•å', 'è…¹æ³»', 'è…¹ç—›', 'æ°”å–˜', 'ç—°', 'å¤´ç—›', 'å’½ç—›'],
            'diagnosis': ['æ„Ÿå†’', 'æ”¯æ°”ç®¡ç‚', 'è‚ºç‚', 'æ¶ˆåŒ–ä¸è‰¯', 'ä¸Šå‘¼å¸é“æ„ŸæŸ“', 'è…¹æ³»', 'æ”¯æ°”ç®¡è‚ºç‚'],
            'treatments': ['åƒè¯', 'æœè¯', 'å°±åŒ»', 'æ£€æŸ¥', 'ä¼‘æ¯', 'å¤šå–æ°´', 'ç‰©ç†é™æ¸©'],
            'examinations': ['è¡€å¸¸è§„', 'èƒ¸ç‰‡', 'å¬è¯Š', 'åŒ–éªŒ']
        }
    
    def preprocess_text(self, text):
        """é¢„å¤„ç†æ–‡æœ¬ï¼Œå¤„ç†ç©ºå€¼å’Œæ— æ•ˆå†…å®¹"""
        if text is None:
            return ""
        
        text = str(text).strip()
        
        # è¿‡æ»¤æ‰åªæœ‰æ ‡ç‚¹ç¬¦å·æˆ–ç©ºæ ¼çš„æ–‡æœ¬
        if not text or text in ["", "æ— ", "ä¸è¯¦", "æš‚ç¼º", "æš‚æ— "]:
            return ""
        
        return text
    
    def calculate_bertscore_safe(self, pred_texts, true_texts):
        """å®‰å…¨çš„BERTScoreè®¡ç®—ï¼Œå¤„ç†ç©ºæ–‡æœ¬"""
        # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
        pred_texts_clean = [self.preprocess_text(text) for text in pred_texts]
        true_texts_clean = [self.preprocess_text(text) for text in true_texts]
        
        # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬å¯¹
        valid_pairs = []
        valid_pred = []
        valid_true = []
        
        for pred, true in zip(pred_texts_clean, true_texts_clean):
            if pred and true:  # åªæœ‰å½“é¢„æµ‹å’ŒçœŸå®æ–‡æœ¬éƒ½ä¸ä¸ºç©ºæ—¶æ‰è®¡ç®—
                valid_pairs.append((pred, true))
                valid_pred.append(pred)
                valid_true.append(true)
        
        if not valid_pairs:
            print("è­¦å‘Š: æ‰€æœ‰æ–‡æœ¬å¯¹éƒ½ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—BERTScore")
            return {
                'precision': 0.0,
                'recall': 0.0, 
                'f1': 0.0,
                'valid_pairs_count': 0,
                'total_pairs_count': len(pred_texts)
            }
        
        try:
            P, R, F1 = self.bertscorer.score(valid_pred, valid_true)
            
            # å¯¹äºæ— æ•ˆå¯¹ï¼Œåˆ†æ•°è®¾ä¸º0
            precision_scores = np.zeros(len(pred_texts))
            recall_scores = np.zeros(len(pred_texts))
            f1_scores = np.zeros(len(pred_texts))
            
            # å¡«å……æœ‰æ•ˆå¯¹çš„åˆ†æ•°
            valid_idx = 0
            for i, (pred, true) in enumerate(zip(pred_texts_clean, true_texts_clean)):
                if pred and true:
                    precision_scores[i] = P[valid_idx].item()
                    recall_scores[i] = R[valid_idx].item()
                    f1_scores[i] = F1[valid_idx].item()
                    valid_idx += 1
                else:
                    # ç©ºæ–‡æœ¬å¯¹å¾—0åˆ†
                    precision_scores[i] = 0.0
                    recall_scores[i] = 0.0
                    f1_scores[i] = 0.0
            
            return {
                'precision': precision_scores.mean(),
                'recall': recall_scores.mean(),
                'f1': f1_scores.mean(),
                'precision_scores': precision_scores.tolist(),
                'recall_scores': recall_scores.tolist(),
                'f1_scores': f1_scores.tolist(),
                'valid_pairs_count': len(valid_pairs),
                'total_pairs_count': len(pred_texts)
            }
            
        except Exception as e:
            print(f"BERTScoreè®¡ç®—é”™è¯¯: {e}")
            return {
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'valid_pairs_count': 0,
                'total_pairs_count': len(pred_texts)
            }
    
    def extract_entities_by_category(self, text):
        """æŒ‰ç±»åˆ«æå–åŒ»ç–—å®ä½“"""
        entities = {category: set() for category in self.entity_categories.keys()}
        
        text = self.preprocess_text(text)
        if not text:
            return entities
        
        for category, keywords in self.entity_categories.items():
            for keyword in keywords:
                if keyword in text:
                    entities[category].add(keyword)
        
        return entities
    
    def calculate_macro_recall(self, pred_entities_list, true_entities_list):
        """è®¡ç®—Macro-Recall"""
        all_categories = list(self.entity_categories.keys())
        category_recalls = {category: [] for category in all_categories}
        
        valid_samples = 0
        
        for pred_entities, true_entities in zip(pred_entities_list, true_entities_list):
            has_valid_entities = False
            
            for category in all_categories:
                # ç¡®ä¿ä½¿ç”¨é›†åˆç±»å‹
                pred_set = set(pred_entities.get(category, [])) if isinstance(pred_entities.get(category), list) else pred_entities.get(category, set())
                true_set = set(true_entities.get(category, [])) if isinstance(true_entities.get(category), list) else true_entities.get(category, set())
                
                if len(true_set) == 0:
                    # å¦‚æœçœŸå®å®ä½“ä¸ºç©ºï¼Œå¬å›ç‡ä¸º1ï¼ˆæ²¡æœ‰éœ€è¦å¬å›çš„å†…å®¹ï¼‰
                    recall = 1.0
                else:
                    # è®¡ç®—è¯¥ç±»åˆ«çš„å¬å›ç‡
                    tp = len(pred_set & true_set)
                    recall = tp / len(true_set) if len(true_set) > 0 else 0.0
                    has_valid_entities = True
                
                category_recalls[category].append(recall)
            
            if has_valid_entities:
                valid_samples += 1
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡å¬å›ç‡
        category_avg_recalls = {}
        for category, recalls in category_recalls.items():
            category_avg_recalls[category] = np.mean(recalls) if recalls else 0.0
        
        # è®¡ç®—Macro-Recallï¼ˆæ‰€æœ‰ç±»åˆ«çš„å¹³å‡å¬å›ç‡ï¼‰
        macro_recall = np.mean(list(category_avg_recalls.values()))
        
        return {
            'macro_recall': macro_recall,
            'category_recalls': category_avg_recalls,
            'detailed_recalls': category_recalls,
            'valid_samples_count': valid_samples,
            'total_samples_count': len(pred_entities_list)
        }
    
    def evaluate_single_pair(self, pred_text, true_text):
        """è¯„ä¼°å•ä¸ªé¢„æµ‹-çœŸå®å¯¹"""
        # é¢„å¤„ç†æ–‡æœ¬
        pred_text_clean = self.preprocess_text(pred_text)
        true_text_clean = self.preprocess_text(true_text)
        
        # BERTScoreè¯„ä¼°
        bertscore_result = self.calculate_bertscore_safe([pred_text_clean], [true_text_clean])
        
        # å®ä½“æå–
        pred_entities = self.extract_entities_by_category(pred_text_clean)
        true_entities = self.extract_entities_by_category(true_text_clean)
        
        return {
            'bertscore': bertscore_result,
            'pred_entities': {k: list(v) for k, v in pred_entities.items()},
            'true_entities': {k: list(v) for k, v in true_entities.items()},
            'pred_text': pred_text_clean,
            'true_text': true_text_clean,
            'is_valid_pair': bool(pred_text_clean and true_text_clean)
        }
    
    def evaluate_field_level(self, pred_report, true_report):
        """å­—æ®µçº§åˆ«çš„è¯„ä¼°"""
        fields = ['ä¸»è¯‰', 'ç°ç—…å²', 'è¾…åŠ©æ£€æŸ¥', 'æ—¢å¾€å²', 'è¯Šæ–­', 'å»ºè®®']
        field_results = {}
        
        for field in fields:
            pred_field = pred_report.get(field, '')
            true_field = true_report.get(field, '')
            
            field_eval = self.evaluate_single_pair(pred_field, true_field)
            field_results[field] = field_eval
        
        return field_results
    
    def comprehensive_evaluation(self, pred_reports, true_reports):
        """ç»¼åˆè¯„ä¼°æ‰€æœ‰æŠ¥å‘Š"""
        print("å¼€å§‹ç»¼åˆè¯„ä¼°...")
        
        # é¦–å…ˆæ£€æŸ¥æ•°æ®è´¨é‡
        self.analyze_data_quality(pred_reports, true_reports)
        
        # å­—æ®µçº§åˆ«çš„BERTScoreå’Œå®ä½“ç»Ÿè®¡
        field_bertscores = defaultdict(list)
        field_entities = defaultdict(lambda: {'pred': [], 'true': []})
        field_valid_counts = defaultdict(int)
        
        # æ•´ä½“æ–‡æœ¬è¯„ä¼°ï¼ˆåˆå¹¶æ‰€æœ‰å­—æ®µï¼‰
        overall_pred_texts = []
        overall_true_texts = []
        overall_pred_entities = []
        overall_true_entities = []
        
        for i, (pred_report, true_report) in enumerate(zip(pred_reports, true_reports)):
            if i % 10 == 0:
                print(f"å¤„ç†ç¬¬ {i+1}/{len(pred_reports)} ä¸ªæ ·æœ¬...")
            
            # å­—æ®µçº§åˆ«è¯„ä¼°
            field_results = self.evaluate_field_level(pred_report, true_report)
            
            for field, result in field_results.items():
                field_bertscores[field].append(result['bertscore']['f1'])
                field_entities[field]['pred'].append(result['pred_entities'])
                field_entities[field]['true'].append(result['true_entities'])
                if result['is_valid_pair']:
                    field_valid_counts[field] += 1
            
            # æ„å»ºæ•´ä½“æ–‡æœ¬
            pred_full_text = " ".join([self.preprocess_text(pred_report.get(f, '')) for f in ['ä¸»è¯‰', 'ç°ç—…å²', 'è¯Šæ–­', 'å»ºè®®']])
            true_full_text = " ".join([self.preprocess_text(true_report.get(f, '')) for f in ['ä¸»è¯‰', 'ç°ç—…å²', 'è¯Šæ–­', 'å»ºè®®']])
            
            overall_pred_texts.append(pred_full_text)
            overall_true_texts.append(true_full_text)
            
            # åˆå¹¶æ‰€æœ‰å­—æ®µçš„å®ä½“
            pred_all_entities = self.extract_entities_by_category(pred_full_text)
            true_all_entities = self.extract_entities_by_category(true_full_text)
            
            overall_pred_entities.append(pred_all_entities)
            overall_true_entities.append(true_all_entities)
        
        # è®¡ç®—æ•´ä½“BERTScore
        print("è®¡ç®—æ•´ä½“BERTScore...")
        overall_bertscore = self.calculate_bertscore_safe(overall_pred_texts, overall_true_texts)
        
        # è®¡ç®—æ•´ä½“Macro-Recall
        print("è®¡ç®—æ•´ä½“Macro-Recall...")
        overall_macro_recall = self.calculate_macro_recall(overall_pred_entities, overall_true_entities)
        
        # è®¡ç®—å­—æ®µçº§åˆ«çš„Macro-Recall
        print("è®¡ç®—å­—æ®µçº§åˆ«Macro-Recall...")
        field_macro_recalls = {}
        for field in field_entities:
            print(f"  å¤„ç†å­—æ®µ: {field}")
            field_macro_recalls[field] = self.calculate_macro_recall(
                field_entities[field]['pred'], 
                field_entities[field]['true']
            )
        
        # æ±‡æ€»ç»“æœ
        summary = {
            'overall': {
                'bertscore': overall_bertscore,
                'macro_recall': overall_macro_recall
            },
            'field_level': {},
            'data_quality': {
                'field_valid_counts': dict(field_valid_counts),
                'total_samples': len(pred_reports)
            }
        }
        
        for field in field_bertscores:
            summary['field_level'][field] = {
                'bertscore_f1_mean': np.mean(field_bertscores[field]),
                'bertscore_f1_std': np.std(field_bertscores[field]),
                'macro_recall': field_macro_recalls[field]['macro_recall'],
                'category_recalls': field_macro_recalls[field]['category_recalls'],
                'valid_pairs_count': field_valid_counts[field],
                'valid_pairs_ratio': field_valid_counts[field] / len(pred_reports)
            }
        
        return summary
    
    def analyze_data_quality(self, pred_reports, true_reports):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("\næ•°æ®è´¨é‡åˆ†æ:")
        print("=" * 50)
        
        empty_pred_count = 0
        empty_true_count = 0
        field_empty_stats = defaultdict(int)
        
        for pred_report, true_report in zip(pred_reports, true_reports):
            # æ£€æŸ¥æ•´ä½“æŠ¥å‘Šæ˜¯å¦ä¸ºç©º
            pred_text = " ".join([str(pred_report.get(f, '')) for f in ['ä¸»è¯‰', 'ç°ç—…å²', 'è¯Šæ–­', 'å»ºè®®']])
            true_text = " ".join([str(true_report.get(f, '')) for f in ['ä¸»è¯‰', 'ç°ç—…å²', 'è¯Šæ–­', 'å»ºè®®']])
            
            if not self.preprocess_text(pred_text):
                empty_pred_count += 1
            if not self.preprocess_text(true_text):
                empty_true_count += 1
            
            # æ£€æŸ¥å„å­—æ®µç©ºå€¼æƒ…å†µ
            for field in ['ä¸»è¯‰', 'ç°ç—…å²', 'è¾…åŠ©æ£€æŸ¥', 'æ—¢å¾€å²', 'è¯Šæ–­', 'å»ºè®®']:
                if not self.preprocess_text(pred_report.get(field, '')):
                    field_empty_stats[f'pred_{field}'] += 1
                if not self.preprocess_text(true_report.get(field, '')):
                    field_empty_stats[f'true_{field}'] += 1
        
        print(f"ç©ºé¢„æµ‹æŠ¥å‘Š: {empty_pred_count}/{len(pred_reports)} ({empty_pred_count/len(pred_reports)*100:.1f}%)")
        print(f"ç©ºçœŸå®æŠ¥å‘Š: {empty_true_count}/{len(true_reports)} ({empty_true_count/len(true_reports)*100:.1f}%)")
        
        for field in ['ä¸»è¯‰', 'ç°ç—…å²', 'è¯Šæ–­', 'å»ºè®®']:
            pred_empty = field_empty_stats.get(f'pred_{field}', 0)
            true_empty = field_empty_stats.get(f'true_{field}', 0)
            print(f"{field}: é¢„æµ‹ç©ºå€¼ {pred_empty}/{len(pred_reports)} ({pred_empty/len(pred_reports)*100:.1f}%), "
                  f"çœŸå®ç©ºå€¼ {true_empty}/{len(true_reports)} ({true_empty/len(true_reports)*100:.1f}%)")

def load_data(results_path):
    """åŠ è½½æ•°æ®"""
    with open(results_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    pred_reports = []
    true_reports = []
    sample_info = []
    
    for item in data:
        # æå–çœŸå®æŠ¥å‘Š
        true_answer_str = item.get('answer', '{}')
        try:
            true_report = json.loads(true_answer_str)
        except:
            true_report = {}
        
        # æå–é¢„æµ‹æŠ¥å‘Š
        model_answer = item.get('model_answer', {})
        if isinstance(model_answer, str):
            try:
                model_answer = json.loads(model_answer)
            except:
                model_answer = {}
        elif model_answer is None:
            model_answer = {}
        
        pred_reports.append(model_answer)
        true_reports.append(true_report)
        sample_info.append({
            'id': item.get('other', {}).get('id', 'unknown'),
            'question': item.get('question', '')[:100] + '...'  # æˆªå–å‰100å­—ç¬¦
        })
    
    return pred_reports, true_reports, sample_info

def safe_convert_numpy_types(obj):
    """å®‰å…¨è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.generic):
        return obj.item()
    elif isinstance(obj, dict):
        return {k: safe_convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [safe_convert_numpy_types(item) for item in obj]
    elif isinstance(obj, set):
        return list(obj)  # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨
    else:
        return obj

def run_evaluation(results_path, output_path):
    """è¿è¡Œå®Œæ•´è¯„ä¼°"""
    print("åŠ è½½æ•°æ®...")
    pred_reports, true_reports, sample_info = load_data(results_path)
    
    print(f"åŠ è½½å®Œæˆ: {len(pred_reports)} ä¸ªæ ·æœ¬")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = MedicalReportEvaluator(model_type="bert-base-chinese")
    
    # æ‰§è¡Œè¯„ä¼°
    print("æ‰§è¡ŒBERTScoreå’ŒMacro-Recallè¯„ä¼°...")
    evaluation_results = evaluator.comprehensive_evaluation(pred_reports, true_reports)
    
    # æ·»åŠ æ ·æœ¬ä¿¡æ¯
    evaluation_results['sample_info'] = sample_info
    evaluation_results['total_samples'] = len(pred_reports)
    
    # ä¿å­˜ç»“æœ
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(safe_convert_numpy_types(evaluation_results), f, ensure_ascii=False, indent=2)
    
    # æ‰“å°è¯„ä¼°æŠ¥å‘Š
    print_evaluation_summary(evaluation_results)
    
    return evaluation_results

def print_evaluation_summary(results):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    print("\n" + "="*80)
    print("åŒ»ç–—æŠ¥å‘Šç”Ÿæˆæ¨¡å‹è¯„ä¼°æŠ¥å‘Š (BERTScore + Macro-Recall)")
    print("="*80)
    
    overall = results['overall']
    field_level = results['field_level']
    
    print(f"\nğŸ“Š æ€»ä½“è¯„ä¼°æŒ‡æ ‡:")
    print(f"  æ ·æœ¬æ•°é‡: {results['total_samples']}")
    print(f"  Overall BERTScore F1: {overall['bertscore']['f1']:.4f}")
    print(f"  Overall Macro-Recall: {overall['macro_recall']['macro_recall']:.4f}")
    
    print(f"\nğŸ“ˆ å­—æ®µçº§åˆ«BERTScore F1:")
    for field, metrics in field_level.items():
        print(f"  {field}: {metrics['bertscore_f1_mean']:.4f} (Â±{metrics['bertscore_f1_std']:.4f})")
    
    print(f"\nğŸ¯ å­—æ®µçº§åˆ«Macro-Recall:")
    for field, metrics in field_level.items():
        print(f"  {field}: {metrics['macro_recall']:.4f}")
    
    print(f"\nğŸ” å®ä½“ç±»åˆ«å¬å›ç‡ (Overall):")
    category_recalls = overall['macro_recall']['category_recalls']
    for category, recall in category_recalls.items():
        category_name = {
            'symptoms': 'ç—‡çŠ¶',
            'diagnosis': 'è¯Šæ–­', 
            'treatments': 'æ²»ç–—',
            'examinations': 'æ£€æŸ¥'
        }.get(category, category)
        print(f"  {category_name}: {recall:.4f}")
    
    print(f"\nğŸ’¡ å…³é”®æŒ‡æ ‡æ€»ç»“:")
    print(f"  âœ… æ•´ä½“è¯­ä¹‰ç›¸ä¼¼åº¦ (BERTScore F1): {overall['bertscore']['f1']:.1%}")
    print(f"  âœ… åŒ»ç–—å®ä½“å¬å›ç‡ (Macro-Recall): {overall['macro_recall']['macro_recall']:.1%}")
    print(f"  âœ… è¯Šæ–­å®ä½“å¬å›ç‡: {category_recalls['diagnosis']:.1%}")
    print(f"  âœ… ç—‡çŠ¶å®ä½“å¬å›ç‡: {category_recalls['symptoms']:.1%}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # RESULTS_PATH = "./results/test_results.json"  # æµ‹è¯•ç»“æœæ–‡ä»¶
    # RESULTS_PATH = "./results/test_results_14b.json"  # æµ‹è¯•ç»“æœæ–‡ä»¶
    RESULTS_PATH = "./results/test_results_7b_wo_few_shot.json"  # æµ‹è¯•ç»“æœæ–‡ä»¶

    # EVALUATION_OUTPUT = "./evaluate_results/bertscore_macro_recall_evaluation.json"
    # EVALUATION_OUTPUT = "./evaluate_results/bertscore_macro_recall_evaluation_14b.json"
    EVALUATION_OUTPUT = "./evaluate_results/bertscore_macro_recall_evaluation_7b_wo_few_shot.json"

    try:
        # æ‰§è¡Œè¯„ä¼°
        evaluation_results = run_evaluation(RESULTS_PATH, EVALUATION_OUTPUT)
        print(f"\nè¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {EVALUATION_OUTPUT}")
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
